
<TITLE> Invisible Web </TITLE>
<H2> Invisible Web </H2>

<H3> Required reading </H3>
<p>
<A href="http://portal.acm.org/citation.cfm?id=1454163">
Google's Deep Web crawl </A> Madhavan et al. 
<em>Proc. VLDB Endowment</em>, 1:2,
August 2008. <br>
This is a central article on the topic, but unfortunately very
poorly written. Do what you can with it. <br>
<A href=
"http://dl.acm.org/citation.cfm?id=2433442"> Crawling deep web entity pages
</A> He et al.
<H3> Other reading </H3>
<A href="http://www-db.cs.wisc.edu/cidr/cidr2009/Paper_115.pdf"> 
Harnessing the Deep Web: Present and Future</A> J. Madhavan et al.,
<em>CIDR (Conference on Innovative Data Systems Research), 2009. </em>  <br>
<A href="http://portal.acm.org/citation.cfm?id=1860708">
Understanding deep web search interfaces: A survey.</A>
Ritu Khare, Yuan An, and Il-Yeol Song, <em> ACM SIGMOD Record</em> 39:1,
March 2010. <br>
<A href="http://www.eecs.umich.edu/~michjc/papers/webtables_webdb08.pdf">
Uncovering the Relational Web,</A> Michael Cafarella et al., WebDB 2008. <br>
<A href="http://citeseer.ist.psu.edu/708823.html">
Structured Databases on the Web: Observations and Implications
</A> Kevin Chang et. al <br>
<A href="http://www.emeraldinsight.com/journals.htm?articleid=1509085">
Indexing the
invisible web: a survey </A> by Yanbo Ru and Ellis Horowitz, 2005. <br>
<A href="http://www.press.umich.edu/jep/07-01/bergman.html">
The Deep Web: Surfacing Hidden Value </A>
by Michael K. Bergman, Journal of Electronic Publishing <br>
 <A href="http://ilpubs.stanford.edu:8090/725/">
Crawling the Hidden Web </A>  
Sriram Raghavan, Hector Garcia-Molina, <em>VLDBM</em> 2001. <br>



<H3> Invisible Web </H3>
<b> Indexable Web: </b> Pages that can be found by crawling  <br>
<b> Invisible Web </b> (Hidden web, deep web):
Information that can be accessed only or primarily by filling out a form.
browser, but will not be found by a standard crawler for various reasons.

<p>  
<b> An enormous invisible Web site,</b> though not a very interesting one: <br>
<A href="http://www.ncdc.noaa.gov/oa/ncdc.html">
NOAA (National Oceanic and Atmospheric Administration)
National Climatic Data Center</A>
Doppler Weather Radar Data: 1600 TBytes (as of 2006) <br>
In 2007 NOAA projected that they will be storing 140 PetaBytes by 2020. <br>
I can't find a current (2011) count.

<H3> Surfacing Deep Invisible Web </H3>
Madhavan et al., "Google's Deep-Web Crawl"

<p>
Basic idea: Execute forms. Get pages. Add to Google index just as if they
were ordinary static web pages. "Surfaced pages contribute results to more
than 1,000 Web-search queries per second on Google.com". Since,
at that time Google received
about 34,000 queries per second, 
then assuming that "contribute results" means "in
a results page of 10 items", that means that 1 out of 340 results is from
the surfaced web.

<p>
Emphasis on getting some data from a lot of sites rather than exhaustively
surfacing a single site.

<p>
When you fill out a search box and hit "search" or "return", what happens
is that the browser constructs a URL that contains the search key as
the value associated with some key. So the problem is to figure out what
the URL is supposed to look like (template generation) and to find 
productive key values (query generation).

<UL>
<LI> Locate HTML forms with action "get".
<LI> Parse the form
<LI> Template generation
<LI> Query generation
<LI> Index results.
</UL>

<P>
<B> Template: </b> When you fill out a form, some of the input fields are <em>
binding </em> (that is, have a specified value) and the rest are <em> free</em>
("Any" option, blank text, wildcard). Some inputs are presentational (e.g.
order); these are unimportant.
A <em>template</em> is a choice of
binding fields. The <em> dimension</em> of the template is the number of 
binding fields. 

<p>
<b> Informativeness: </b> A <em>signature</em> for a results page is a way of
computing whether two pages are the same: Ignore HTML formatting, ads, order
of results. Delete query words from page (just echoing).
Queries that give HTML errors or give failed search messages 
are ignored. The absolute <em>informativeness</em> of template T is 
(the number of distinct signatures returned in queries generated by T) /
(the number of form submissions on T). If the informativeness is greater than
0.2 then T is informative.

<p>
The incremental informativeness is the number of <em> new</em> distinct
signatures divided by the number of queries.


<p>
<b> Generating templates </b> <br>
Try all 1-dimensional templates. <br>
For each, issue a lot of queries. <br>
See which templates are informative. <br>
Extend all the 1-D templates to 2-D templates <br>
Iterate. Stop after 3-D templates. 

<p>
<b>Generating queries.</b> Here things become murky.
<UL>
<LI>
Finite choice (radio button, drop down menu, etc.). Try all.
<LI>
Typed text box. Known collection of types. US Cities, Zip Code, Lowest-Price,
Highest-Price, Zip Code. It is not clear to me whether these are all the
types that the system actually uses or just those for which they did some
careful experimentation. Judge type from (a) associated name (b) values that
work.
<LI> Generic text box. Get a seed set of query words off the front form itself.
Issue queries, mine the results pages for words with high TF-IDF, which do
not appear on every results page and which appear on more than one results
page. Iterate.
</UL>

<H3> Crawling Deep Web Entity Pages </H3>
Objective: Find <em> entity</em> pages on deep web sites; i.e. a page about
a single entity (often for sale). Goal is a 
representative sample, rather than an exhaustive search.  

<p>
Parsing the form. As in Madhavan, etc.

<p>
<b> URL template generation </b>
<p>
99% of the time, an entity-oriented sites has a search box on its home page.
<p>
Usually suffices to fill out <em>just</em> the field corresponding to
the search text, and let any other fields take on default values. The
site is usually fine with that.


<p> <b> Query generation </b>
<p>
Search terms extracted from Google query logs (the authors were at Google). 
Looked for records of the form 
< keyword_query, url_clicked, num_times_clicked >
(At first I thought there was a chicken and egg problem here: Google search
must have ebay.com  already associated with "iPhone 4" in order to have
returned the result, so what is the point of using the query log? The
answer is that vanilla Google search associates ebay.com with 
<em>all</em> the phrases of text on the web site equally. The point of
using the query log is that users are more likely to query on the entity
name than on some other random phrase that appears on the page.)
<p>
<b> Problem:</b> Queries often have extra words "cheap iPhone 4", "hp
touchpad review", "price of sony vaio". These have to be excluded before
the web site will accept the query. <b> Answers: </b> (a) Match words with
entities in Freebase. (b) Exclude common extra phrases.
<p>
<b> Expansion </b> Having found some query words (e.g. city names, song
titles, etc.) use Freebase to find other words in the same category.

<p> <b> Empty page filtering </b>
A lot of the queries that are generated with have no results, and these
need to be filtered out.  
<p>
Though empty results pages differ from one site to another, within
a single site they are almost always extremely similar or identical except
for quoting the failed search.
<p>
Therefore, generate a couple of searches that are essentially certain
to fail e.g. xyzxxyzyxxyzyzx; extract the common elements in the two
pages received; and use those to mark any future results as empty.

<p> <b> Second level crawl </b>
The results page from the search is likely to have links to further pages.
Follow these but no further.

<UL>
<LI> 
Site may do query expansion. E.g. a search on "iPhone 4" may include
suggested alternative queires like "iPhone 4 case" and "iPhone 3gs".
<LI> Disambiguation page.
<LI> Faceted search. E.g. search on "camera" and get list of options
under "categories", "brands", "prices".
</UL>
Use only URL's that contain a search key. <br>
Deduplicate: Use only one URL per search key, not URL's with the same
search key that differ in some other parameters e.g. sorting criterion.
(Note that content-based deduplication will not achieve this.)




