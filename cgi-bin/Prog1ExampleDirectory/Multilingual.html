<TITLE> The Multi-Lingual Web</TITLE>
<H2> The Multi-Lingual Web</H2>

<H4> Lots of Languages </H4>
As of April 2015:
There are 7102 known living languages according to Ethnologue, of which
somewhat fewer than 1000 have a written form used by the native population 
(as opposed to linguists, anthropologist, missionaries, etc.) 916 are "dying";
1531 are "In trouble".
Parts of
the Bible have been translated into 3168+ languages, the full Protestant
canon into 457 languages (July 2010).
Library of Congress advanced search supports 480 language categories (a couple
of dozen of these are not individual real languages.)
In 
<A href="http://meta.wikimedia.org/wiki/List_of_Wikipedias#10.2B_articles">
Wikipedia,</A> there are 288 languages. (In the case of #6 Waray-Waray 
with 1,259,050 
articles and #8 Cebuano with 1,208,916, the articles were 
<A href="http://en.wikipedia.org/wiki/Waray_Wikipedia"> created by bots </A>
and therefore presumably poor quality samples of the language.)
Arika Okrent's  <A href="http://inthelandofinventedlanguge.com"> In the Land
of Invented Languages</A> lists 500 invented languages of varying
degrees of completeness and reality, out of a list
of more than 900 in Aleksander Dulichenko, <em>International Auxiliary
Languages</em> 1990. 

<p>
<A 
href="http://stats.wikimedia.org/wikimedia/animations/growth/AnimationProjectsGrowthWp.html">
Cute animation of growth of Wikipedia in different languages </A>

<p>
Of course, individuating languages is often vague, and often
done more on political than linguistic grounds. Estimating the number
of native speakers is very difficult, partly a matter of arbitrary
definition, and also carries political baggage. So even within this
web page there are discrepancies.

<p>
<A href="http://www.ethnologue.com/"> Ethnologue: Languages of the World </A>
Online reference work for languages. <br>
<A href="http://www.omniglot.com/"> Omniglot:</a> The online encyclopedia of
writing systemrs and languages.


<H3> Estimating diversity </H3>

<A href="http://jcmc.indiana.edu/vol12/issue4/gerrand.html">
Estimating Linguistic Diversity on the Internet </A>
Peter Gerrand

<p>
Google (2015) indexes 46 languages. Google Translate, on the other hand, 
supports 90 different languages.
Only 12 are non-European. 
Bing (2015) indexes 36 (fewer than in 2011). 

<p> <b> Two estimates </b> <br>
1. Google search on language "without xcvb" (12/2007). Google no longer 
supports this (2011).


<p>
2. Estimate by Mas, based on AllTheWeb, 2003, cited in 
<A href="http://jcmc.indiana.edu/vol12/issue4/gerrand.html">
Estimating Linguistic Diversity on the Internet </A>
Peter Gerrand


<p>
All counts in thousands of pages 

<p align=center>
<table border=1> 
<tr> <th> Language <th> Google count <th> Google % <th> Mas count <th> Mas %
<tr> <td> Afrikaans <td> --- <td> --- <td> 116 <td> 0.005
<tr> <td> Albanian <td> --- <td> --- <td> 53 <td> 0.003
<tr> <td> Arabic <td> 6,390 <td> 0.25 <td> 2,470 <td> 0.12
<tr> <td> Armenian <td> 1,770 <td> 0.072 <td> --- <td> ---
<tr> <td> Basque <td> --- <td> --- <td> 155 <td> 0.007
<tr> <td> Belarusian <td> 2,720 <td> 0.10 <td> 536 <td> 0.025
<tr> <td> Bulgarian <td> 2,820 <td> 0.11 <td> 1,120 <td> 0.053
<tr> <td> Catalan <td> 785 <td> 0.031 <td> 2,930 <td> 0.14
<tr> <td> Chinese  <td> 353,000 <td> 14.5 <td> 65,700 <td> 3.1
<tr> <td> Croatian <td> 2,520 <td> 0.09 <td> 1,670 <td> 0.08
<tr> <td> Czech <td> 31,700 <td> 1.2 <td> 15,600 <td> 0.73
<tr> <td> Danish <td> 7,470 <td> 0.29 <td> 12,100 <td> 0.57
<tr> <td> Dutch <td> 23,600 <td> 0.97 <td> 41,100 <td> 1.9
<tr> <td> English <td> 1,430,000 <td> 58.6 <td> 1,280,000 <td> 60.4
<tr> <td> Esperanto <td> 357 <td> 0.014 <td> --- <td> ---
<tr> <td> Estonian <td> 875 <td> 0.035 <td> 1,460 <td> 0.069
<tr> <td> Faroese <td> --- <td> --- <td> 66 <td> 0.003
<tr> <td> Filipino <td> 382 <td> 0.015 <td> --- <td> ---
<tr> <td> Finnish <td> 5,610 <td> 0.22 <td> 5,680 <td> 0.27
<tr> <td> French <td> 65,500 <td> 2.68 <td> 99,700 <td> 4.7
<tr> <td> Friesian <td> --- <td> --- <td> 63 <td> 0.003
<tr> <td> Galician <td> --- <td> --- <td> 274 <td> 0.013
<tr> <td> German <td> 124,000 <td> 5.1 <td> 182,000 <td> 8.6
<tr> <td> Greek <td> 2,990 <td> 0.11 <td> 2,370 <td> 0.11
<tr> <td> Hebrew <td> 3,220 <td> 0.12 <td> 4,790 <td> 0.23
<tr> <td> Hungarian <td> 16,700 <td> 0.69 <td> 8,540 <td> 0.40
<tr> <td> Icelandic <td> 518 <td> 0.020 <td> 1,390 <td> 0.066
<tr> <td> Indonesian <td> 2,620 <td> 0.10 <td> 1,040 <td> 0.049
<tr> <td> Italian <td> 23,100 <td> 0.95 <td> 41,800 <td> 1.98
<tr> <td> Japanese <td> 96,400 <td> 4.0 <td> 69,700 <td> 3.3
<tr> <td> Korean <td> 16,300 <td> 0.67 <td> 64,600 <td> 3.0
<tr> <td> Latin <td> --- <td> --- <td> 137 <td> 0.006
<tr> <td> Latvian <td> 854 <td> 0.035 <td> 560 <td> 0.026
<tr> <td> Lithuanian <td> 1,860 <td> 0.077 <td> 1,080 <td> 0.053
<tr> <td> Malay <td> --- <td> --- <td> 348 <td> 0.015
<tr> <td> Norwegian <td> 4,610  <td> 0.18 <td> 8,120 <td> 0.383
<tr> <td> Persian <td> 1,110 <td> 0.044 <td> --- <td> ---
<tr> <td> Polish <td> 64,600 <td> 2.6 <td> 22,200 <td> 1.0
<tr> <td> Portuguese <td> 5,860 <td> 0.25 <td> 37,700 <td> 1.78
<tr> <td> Romanian <td> 6,740 <td> 0.27 <td> 2,000 <td> 0.097
<tr> <td> Russian <td> 47,600 <td> 1.9 <td> 42,300 <td> 2.0
<tr> <td> Serbian <td> 1,030 <td> 0.041 <td> 43 <td> 0.002
<tr> <td> Slovak <td> 4,380 <td> 0.17 <td> 5,080 <td> 0.24
<tr> <td> Slovenian <td> 983 <td> 0.040 <td> 1,690 <td> 0.08
<tr> <td> Spanish <td> 37,900 <td> 1.6 <td> 65,800 <td> 3.1
<tr> <td> Swahili <td> --- <td> --- <td> 14 <td> 0.001
<tr> <td> Swedish <td> 9,430 <td> 0.38 <td> 14,900 <td> 0.7
<tr> <td> Thai <td> 3,610 <td> 0.14 <td> 3,120 <td> 0.15
<tr> <td> Turkish <td> 23,500 <td> 0.97 <td> 4,700 <td> 0.22
<tr> <td> Ukranian <td> 990 <td> 0.040 <td> 1,010 <td> 0.048
<tr> <td> Vietnamese <td> 2,610 <td> 0.10 <td> 390 <td> 0.018
<tr> <td> Welsh <td> --- <td> --- <td> 93 <td> 0.004
<tr> <td> Total <td> 2,437,000 <td> <td> 2,118,500
</table>
</p>
<p>
Note that the Google total is nothing like the number of pages Google 
presumably actually indexed in 2007 (at least 8 billion).

<!---
<p>
Quite a number of languages dropped in total number of pages retreived.
Particularly startling are Korean, which drops by a factor of 4; 
Norwegian which drops by a factor of 2; German, which drops from
182 million to 124 million pages, and French, which drops from 99,700 to
65,500. Presumably none of these are real, but one wonders what if anything
they correspond to, and what the real percentage is. 

<p>
Some languages increase startlingly; e.g. Serbian increases by a factor of
more than 20, and Vietnamese and Turkish increases by a factor of 5.
Hard to know whether this is real or artifactual.
--->

<p>
Of the 20 languages with the 
<A href="
http://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers">
largest speaker populations </A>
6 were not on this list: (Note: these counts are very problematic.) <br>
Hindi (310 M speakers: #4)  <br>
Bengali (205 M speakers #7) <br>
Punjabi (102 M speakers: #10) <br>
Javanese (82 M speakers: #12) <br>
Telugu  (76 M speakers: #15) <br>
Marathi (73 M speakers: #19) <br>
(In 2011, Hindi is indexed by Google.
For Hindi and Telugu, see Kilgariff et al. below),

<p>
As of 2015, Hindi has 101,922 Wikipedia articles (#51); Bengali has 
35,406 (#80); Western Punjabi has 33,822 (#84) and Punjabi has 16,764 (#107);
Javanese has 48,418 (#74); Telugu has 69,064 (#68); and
Marathi has 41,722 (#76).

<p>
In 1997 and 2002 two experiments were carried out by pinging randomly 
generated IP addresses and testing the language of the home page. Unlike any
of the other measures here, that is the only one that is genuinely random; the
rest are  based on search engines that have who knows what kind of bias. 
However, it only measures what it measures, not the fraction of Web pages.
As far as I can find out, no such experiment has been carried out since.


<p>
You would think that UNESCO would do this kind of thing. But their 2009
paper 
<A href="http://unesdoc.unesco.org/images/0018/001870/187016e.pdf">
Twelve years of measuring linguistic diversity on the Internet: Balance
and Perspective" </A>
by Daniel Pimienta, Daniel Prado, and Alvaro Blanco, discusses only eight
languages --- English, Spanish, French, Italian, Portuguese, Romanian,
German, and Catalan -- and is more interested in scoring political points.
They don't seem to have done anything since.

<H3> Web Search in Other Languages </H3>
<p>
<A href="http://www.springerlink.com/content/5k5h03h35515l64m/">
Current research issues and trends in non-English Web searching </A>
Fotis Lazarinis, Jes&uacute;s Vilares, John Tait and Efthimis N. Efthimiadis
(2009). <em>Information Retrieval </em>
Volume 12, Number 3, 230-250, DOI: 10.1007/s10791-009-9093-0 

<H4> Document processing </H4>

<p> <b> Language identification: </b> Easy to get 100% accuracy in 
distinguishing French from English. Difficult for (a) languages which are
very similar; (b) languages where there is only a small corpus. 

<p> <b> Alphabetic encoding: </b> Very boring, very difficult. Multiple 
encodings for the same language. 95% of Indian language content is
not searchable, largely for this reason. Many encodings are proprietary.
In Greek, the upper case letters get confounded with the corresponding
Latin upper case letters, which is fine, until you regularize to lower-case; 
"&Alpha;&Beta;&Alpha;&Kappa;&Alpha;&Sigma;" gets regularized to
"abaka&sigma;" (mixture of Latin and Greek) instead of
"&alpha;&beta;&alpha;&kappa;&alpha;&sigma;". 
File names tend to be in Latin alphabet, because operating systems are less
enlightened than search engines; this makes havoc for image search, which
relies substantially on file names.

<p>
Just cutting and pasting "Jes&uacute;s" from the biliographic entry into
this HTML file got into trouble; it came out as 
"Jes√∫s"

<p>
Dealing with Unicode is apparently one of the major timesinks in international
web service development of any kind.  

<p>
<b> Text segmentation:</b> Discussed in lecture 2.
Even in English, breaking at white space and punctuation is not a reliable
rule. In other languages, much worse. (A) No white space between words
(Chinese, other Asian languages). (B) Heavy use of compound words (Dutch,
German, etc.). One alternative that is been suggested is just to give up on
words altogether and allow search on arbitrary sequences of letters.

<p> <b> Stopword list, stemming, regularization </b> Discussed in lecture 2.

Of course each language presents its own issues, and the effectiveness of
different strategies is variable.

<p> <b> Crawling:</b> One could imagine that a crawler specialized for a
particular minority language could do a better job than Google, but the
presumption is mostly that that is not the case. Most efforts to collect
corpora for minority languages piggyback off Google.


<H4> Queries </H4>
Google gives three ways of e.g. getting pages in German. 
<UL>
<LI> Specify German in "Advanced Search"
<LI> Set "German" as your preferred language for results in "Preferences".
<LI> Go to the German Google server 
<A href="http://www.google.de"> www.google.de </A>
</UL>
Moreover, Google offers its interface about 149 languages (a few of these
are not languages; e.g. "Elmer Fudd").  


<p>
Lewandowski (2008) in
<A href=
"http://www.emeraldinsight.com/journals.htm?articleid=1747662&show=abstract">
"Problems with the use of web search engines to find results in foreign 
languages", </A>
states that if you use www.google.de, and restrict search to English
language documents, only 46% of the results are actually in English. MSN
search was worse, at 34%; Ask.com and Yahoo were fine, at 94% and 95%.
Hard to understand why this should be so bad. I was not able to replicate
in a few tries. 

<p>
Many studies of search in non-English languages. Two major questions:
<UL>
<LI> Are the large search engines offering adequate search facilities? 
Answer is generally no, even for Indo-European languages, and worse for
more distant languages.

<LI> Do language-specific search engines do better? Sometimes:
Yes in the case of Chinese (Sohu and Baidu), Russian, and Hungarian
(see 
<A href="http://www2003.org/cdrom/papers/alternate/P415/415.pdf">
How do search engines handle non-English queries? A case study
</A> Judit Bar-Ilan and Tatyana Gutman 2003).
No for Turkish, Greek, Italian.
</UL>

<H3> Collecting Corpora for Languages </H3>
<p>
<A href="http://borel.slu.edu/pub/wac3.pdf">
The Crubadan Project: Corpus building for under-resourced languages
</A> Kevin Scannell

<p>
Collected corpora for 416 living languages from the Web.

<p>
<A href="http://borel.slu.edu/crubadan/stadas.html">
Crubadan 2.0: Status Page
</A> Kevin Scannell


<BLOCKQUOTE>
Only a very small number (perhaps thirty) of the world's 6000+ living languages
enjoy the benefits of modern language technologies such as speech recognition
and machine translation.  A slightly larger number (less than 100) have managed
to assemble the basic resources needed as a foundation for advanced end-user
technologies: monolingual and bilingual corpora, machine-readable dictionaries,
thesauri, part-of-speech taggers, morphological analyzers, parsers etc.
</BLOCKQUOTE>

<UL>
<LI> Prepare metadata
<LI> Identify character set(s) and anomalies
<LI> Collect and clean training texts.
<LI> Web search for documents in language
<LI> Classify documents.
</UL>

<p> <b> Metadata </b> <br>
Name of language in English, ISO 693-3 code (standard 3 letter identifier 
for lang.) polluting languages (e.g. Spanish
is a polluter of Basque, English is a polluter by default).  <br>
Additional meta-data by screen scraping Ethnologue site.

<p>
<b> Character set </b> <br>
Presumably there is a Unicode standard, but for many languages there are large
numbers of non-standard usages (legacy or otherwise) -- see paper.
Generally necessary to consult with speaker.

<p>
<b> Training text: </b>
<UL> <LI> Wikipedia, Jehovah's witnesses, and UN Universal
Declaration of Human Rights site. 
<LI> Text provided by native-speaking contributors. 
<LI> Open-source spelling corrector (hence a word list). 
<LI> Filter to produce clean word list
<UL>
<LI> Words with characters not in target language
<LI> Words with no vowels (except in languages written with no vowels)
<LI> Words with same character three times in a row
<LI> Words with capital letter in the middle.
<LI> Words from a polluting language
<LI> Words with no diacritic if same word appears more frequently with a 
diacritic.
<LI> Language-specific filters. E.g. Hawaiian words never have two consonants
in a row.
</UL>
<LI> Collect trigrams (sequences of three characters).
<LI> Further filter out words with improbable trigrams
<LI> Identify nearby languages by comparing vector of trigram frequencies
<LI> Extract high-frequency words that are not high-frequency in other
languages.
<LI> Define tokenizer
</UL>

<p> <b> Collect documents in language from Web </b>
Construct search engine query with random words in lexicon OR'd together
then ANDed with one stop word.

<p>
This often gives high precision in retrieving docs in target language, e.g.
98% for Irish.  Sometimes it is hard to find a good stopword that is not
also common in other languages.

<p>
Download docs, convert to plain text UTF-8.

<p> Apply language recognizer. Angle cosine between vector of trigrams.

<p> If doc is in target language, then use as seed for crawler. <br>
If doc is in nearby language, use as seed for crawler for that language. <br>
Otherwise, discard.

<p> Expand lexicon for further queries.

<p> Continue until either (a) enough document retrieved or (b) no new URLs,
no new queries.

<p> <b> Issues </b>
<UL>
<LI> Sometimes worth recording paragraphs in target language. 
<LI> With very close languages families:
<UL>
<LI> Collect documents in family as above.
<LI> Give to expert for manual labelling
<LI> Train Naive Bayes classifier at the word level.
</UL>
<LI> With multiple orthographies (Cornish), use separate classifiers.
</UL>

<p> <b> Applications </b>
<UL>
<LI> Open source spell checkers
<LI> Morphological analysis
<LI> Collection for lexicography. 100,000,000 million words of Welsh
provided to U. Wales Welsh Dictionary project.  Inclusiveness important.
</UL>

<H4> <A href="http://www.lrec-conf.org/proceedings/lrec2010/pdf/79_Paper.pdf">
A Corpus Factory for many languages </A></H4>
Adam Kilgariff et al. <em> LREC</em> 2010.

<p> Gather large corpora for eight languages: Dutch, Hindi, 
Indonesian, Norwegian, Swedish, Telugu, Thai, Vietnamese.

<UL>
<LI> 1. Gather a seed word list of several hundred mid-frequency words.
<LI> 2. Repeat several thousand times, or until corpus is large enough:
<UL>
<LI> Randomly select three words for query. (Three depends on language;
may be 2 or 4).
<LI> Send query to commercial search engine: use Yahoo and Bing API's. 
Why not Google?
Google used to offer an API for invoking the search engine from inside
programs. But (a) Google no longer offers it, though if you have it, you
can continue to use it (b) it was limited in the maximum number of
queries per day.
<LI> Download and store pages
</UL>
<LI> 3. "Clean" the text.
<LI> 4. Remove duplicates
<LI> 5. Tokenize. If tools are available, lemmmatise and part-of-speech
tag.
<LI> 6. Index in queriable database.
</UL>

<p><b> Seed word selection.</b> Get Wikipedia corpus. Filter stubs etc.
Tokenize. Except Vietnamese, exclude words of fewer than 5 letters (too
likely to also be words of some other language). 
Use top 1000 words as "high-frequency" words and next 5000 words
as "mid frequency words". 

<p> <b> Cleaning the text:</b> Again, a dull but major issue throughout
this kind of work. Remove HTML markup, navigation bars, boilerplate.
Boilerplate is identified as high ratio of tags to text at the beginning
and end of a page (difficulties in the case of pages with boilerplate
in the middle). 

<p>
Eliminate word lists, identified as pages with too few function words. 
(Queries on 3 random mid-frequency pages tend to turn them up.) 

<p> <b> Eliminate duplicates, near duplicates </b> Perl Text::DeDuper module.
Uses shingle method, as discussed earlier in the semester.

<p> <b> Results </b>
<p align="center">
<table border="1">
<tr> <th> <th> URLs <th> Filtered <th> Dups Removed  <th> MB <th> m Words
<th> Wiki Corpus
<tr> <th> Dutch <td align=right> 97,584 <td align=right> 22,424 <td align=right> 19,708 <td align=right> 739 <td align=right> 108.6 
<td align=right> 30.0 
<tr> <th> Hindi <td align=right> 71,613 <td align=right> 20 051 
<td align=right> 13,321 <td align=right> 424 <td align=right> 30.6 
<td align=right> 2.5 
<tr> <th> Indonesian <td align=right> 78,402 <td align=right> 28,987 
<td align=right> 27,051 <td align=right> 708 <td align=right> 102.0
<td align=right> 8.5 
<tr> <th> Norwegian <td align=right> 258,009 <td align=right> 66,299 
<td align=right> 62,691 <td align=right> 628 <td align=right>
94.9 <td align=right> 19.1 
<tr> <th> Swedish <td align=right> 168,511 <td align=right> 31,683 
<td align=right> 28,842 <td align=right> 719 <td align=right> 114.0
<td align=right> 9.3 
<tr> <th> Telugu <td align=right> 37,864 <td align=right> 6,178 
<td align=right> 5,131 <td align=right> 107 <td align=right> 3.4 
<td align=right> 0.2
<tr> <th> Thai <td align=right> 120,324 <td align=right> 23,320 
<td align=right> 20,998 <td align=right> 1200 <td align=right> 81.8
<td align=right> 6.2
<tr> <th> Vietnamese <td align=right> 106,076 <td align=right> 27,728 
<td align=right> 19,646 <td align=right> 1200 <td align=right>
149.0 <td align=right> 9.5
</table>
</p>


As comparted to the Wiki corpus, the Web corpus is less "informational" and
more "interactional" (personal pronouns "I" and "you" are much more 
frequent.)

<H3> <A href="http://triceratops.brynmawr.edu/dspace/bitstream/handle/10066/15841/Nilsson_thesis_2015.pdf"> 
A Snowball Sampling Approach for Studying Digital Minority Languages,</A>
</H3>
Peter Nilsson, BA Thesis, Linguistic Dept., Swarthmore College.

<p>
Distinctions: 

<p>
<em>Language presence</em> (you can find some examples of the language
on the internet) vs. <em> language use</em> (a speaker of the language
has put content on the web in the language with the intent of 
communicating with some other speaker).

<p>
<em>Unidirectional </em> (web pages) vs. <em>multidirectional</em>
(emails, messages, tweets etc.)

<p>
<b> Categorization of digital minority presence types </b>
<UL>
<LI> <em> Coincidental presence.</em> Online occurrences of a language
that were not intentionally produced for the internet. E.g. videos containing
speech that were later uploaded. This can be hard to find online, since it
may not be indexed by the language.
<LI> <em> Description presence.</em> There are linguistic descriptions
of the language on the web (excluding books that were originally
in print and later uploaded). This is generally easy to find online,
since the language is highlighted.
<LI> <em> Educational presence.</em> Content intended to teach the
language. Easy to find online, for the same reason.
<LI> <em> Utility presence</em> Content in the language about some
subject other than the language, created for the web, intended to
be useful. Can be hard to find online.
</UL>

<H4> Case studies </H4>
<p> <b> Languages with a minimal presence.</b> <br>
Yokoim (Papua New Guinea). 2000 speakers. Some descriptive presence. <br>
Kanuri (Nigeria and Niger). 3.7 million speakers. Very small 
coincidental presence (Youtube videos) of unclear quality. Hard to find
online, in part because, as it happens, many of the words strongly
associated with the language have other meanings.

<p> <b> Languages with some use. </b> <br>
Niuean (Austronesia). About 10,000 speakers. Internet communication
used by a diaspora in New Zealand (e.g. Facebook groups). Some Youtube videos.

<p> 
Anishinaabemowin (aka Ojibwe). 4th most populous Native American group in 
US, 2nd most populous in Canada. About 90,000 speakers, mostly over sixty
years old. No one learns Ojibwe as a first language any more.
Mostly educational presence. There is discussion, web pages, etc.; but it
is mostly in English about the language rather than in the language.

<p> <b> Languages with regular use </b> <br>
Inuktitut. Inuit language in Canada. About 30,000 people. There is
substantial discussion in the language itself. The language and its web
presence has support from the Canadian government.




<H3>
<A href=
"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0077056">
Digital Language Death.</A></H3>
A. Kornai (2013) PLOS One. Gloomy and not extremely clear.

<p>
Somewhat strange presumption and metaphor: A language which does not yet have 
much of 
a presence on the internet is "no longer capable of digital ascent" 
and has thus suffered "Digital internet death" (without, in most cases, having
been born digitally).

<p><b>Method</b>
<UL>
<LI> As the source for small languages, he pretty much relies on Crubadan.
<LI> Define a whole collection of indicators e.g. Is the language supported
by Microsoft Word? 
<LI> Define four categories: Thriving, Vital, Heritage (essentially
dead language, substantially discussed but hardly used), and Still (small
digital presence, presumed non-viable).
<LI> For each category choose a set of prototypical examples.
<LI> Use these prototypical examples as the training set for a maximum
entropy classifier. Use cross-validation to verify the accuracy of the 
classifier.
<LI> Run the classifier over the whole data.
</UL>

<p>
<b> Gloomy conclusions:</b>
<UL>
<LI> 16 languages are Thriving: English, Japanese, French, German, Spanish,
Italian, Portuguese (Brazilian and European), Dutch,  Swedish, Norwegian,
Danish, Finnish, Russian, Polish, Chinese (Traditional and Simplified), and 
Korean.  (These are the languages with OS-level support by Apple.)
<LI> 252 languages are Vital.
<LI> 162 languages are Borderline. (Classification unclear, mostly due to
sparse data).
<LI> 51 languages are Heritage.
<LI> 307 languages are Still.
<LI> The remaining 8000 have no detectable internet presence.
</UL>
Kornai considers these figures optimistic.
<p>
The digital status correlates closely with the established estimate of the
viability of the spoken language, unsurprisingly.


<p>
<b> Interesting conclusions:</b>
<p>
The best indicator is the "adjusted Wikipedia size"; the number of Wikipedia
pages that are more than K characters long, where K is a function of the
language (some languages use more characters than others for the same content).
"No wikipedia, no ascent". (Kornai has done other work on Wikipedia, and I'll
give pretty long odds is a Wikipedia editor, though I can't verify that.)
 

<H3><A href="http://www.kornai.com/Papers/wac4.pdf">
Google for the Linguist on a Budget </A></H3>
Andr&aacute;s Kornai and P&eacute;ter Hal&aacute;csy,
<em> LREC Web as Corpus Workshop Proceedings,</em> 2008.

<p>
Nice brief account of a (comparatively) light-weight crawler and 
query-server. However what is most interesting is the contrast in objectives
with standard web search engines (i.e. those being used by people seeking
information rather than linguistic phenomona):

<UL>
<LI> No value on currency.
<LI> No value on completeness, as long as largeness is achieved. This enables
various kinds of efficiency/quality choices. E.g. once a URL has failed,
it gets placed on a "forbidden" list and not tried again.
<LI> Premium on linguistically high-quality  sites.
<LI> Repeatability of data set is important. If I have a new method and I
want to compare it to a method that was used 2 years ago, I want to compare
it on the same data. Therefore, nothing is discarded, and everything is
time-stamped.
</UL>

<p>
System achieves 330 GBytes/day.


<p>
Collect a large corpus of documents in a language, for linguistic study,
subject to the following constraints:
<UL>
<LI>1. Run on commodity hardware (less than $15K per node).
<LI>2. Hold a useful number of pages (one billion per node)
<LI>3. Provide facilities for logging, checkpointing, repeating, and
balancing subtasks
<LI>4. Support one million queries per day.
<LI>5. Not be a drain on external resources/goodwill.
</UL>

<H3> Collecting a parallel corpus (bitext) </H3>
<p>
<A href="http://portal.acm.org/citation.cfm?id=964751.964753">
The Web as a parallel corpus </A> Philip Resnik and Noah A. Smith

<p>
Task: Collect pairs of web pages that are translations of one another
in different languages.

<UL>
<LI> Locating parent and sibling pages
<LI> Generating candidate pages at promising sites
<LI> Structural filtering
</UL>

<p> <b> Parent and sibling pages </b>
"A parent page page is one that contains hypertext links to different-language
versions of a document." <br>
Altavista query: (anchor:"english" OR anchor:"anglais") AND
(anchor:"french" OR anchor:"francais").  Extract those in which these anchors
are within 10 lines of one another.

<p>
"A sibling page is a page in one language that itself contains a link to a 
version of the same page in another language. <br>
Request pages in French that match the expression "anchor:"english" OR
anchor:"anglais".

<p> <b> Candidate pages at promising sites </b>
Having found a promising site --- e.g. from the search for parents and siblings
or from known sites like government sites in Canada --- crawl the site.
Then match URL's using manually constructed substitution rules.
E.g. if you have an English-Chinese site and you have a URL <br>
http://mysite.com/english/index.html then look for 
http://mysite.com/chinese/index.html. <br>
If you have a URL http://mysite.com/index_en.html look for site
http://mysite.com/index_ch.html.  And so on.

<p> <b> Structural filtering</b> 
The presumption is that (a) when the text on the page is translated,
the HTML structure will be left more or less the same; (b) that the
translations are of reasonably equal lengths.
<p>
Given a candidate pair of pages, linearize the HTML (matching strings is
easier than matching trees) and use dynamic programming to find the optimal
alignment.  Characterize the quality of alignment in terms:
<UL>
<LI> Difference percentage: the fraction of alignment tokens that are in
one text but not both.
<LI> Correspondence of lengths of aligned text chunks.
</UL>

<p> <b> Results: </b> <br>
English/French: Precision=100%, Recall=68.6%.
English/Chinese: Precision=98%, Recall=61%.
English/Spanish: Precision=100%, Recall=60%

<H3> The Linguists Playground </H3>
<p>
<A href="http://www.umiacs.umd.edu/users/resnik/pubs/bls2005.pdf">
The Web in Theoretical Linguistics Research: Two Case Studies using
the Linguist's Search Engine
</A> Philip Resnik et al.


<p>
Toy example: Look for sentences where "titrates" is used intransitively.

<p align=center>
<img src=
"http://cs.nyu.edu/courses/spring11/G22.2580-001/lse.gif">
</p>

<p>
User inputs sample sentence "John titrates the solution". <br>
Program generates and displays parse tree. <br>
User edits parse tree.
Program interprets as query (S NP (VP (VBZ titrates+VERB) (! NP))) <br>
(I.e. "Look for any clause (S) with an NP followed by a VP where the
verb is any form of "titrates" and where there is no NP as object.")

<p>
LSE has an existing corpus of several million parsed sentences. <br>
Alternatively, the user can request that it: <br>
search the web under "titrate",  <br>
extract all sentences containing "titrate",  <br>
parse and index these sentences, <br>
return sentences satisfying the specification.

<p>
There are actually some:
"You titrate with sulfuric acid using gloves and safety glasses" etc.

<p> <b> Linguistically interesting example 1: </b>
Comparative correlative with "then"
<p>
A "comparative correlative" is the construction like "The more pizza Romeo
eats, the fatter he gets".  This is thought to be problematic for Chomsky's
theory of Universal Grammar --- don't ask me.

<p>
However the argument loses weight (I think --- I may be backwards on this)
if examples of the construction can be
found where the second part is introduced with "then". Some examples are
indeed found on the web:

<p>
"The darker the coffee bean, then the less caffeine." <br>
"The more playing time in the past, then the less regression to the mean 
needed"

<p> <b> Psycho-linguistically interesting example 2: </b>
For purposes of designing an experiment, it was desired to collect natural
examples of sentences like "It [was/is/seemed ...] 
[clear/obvious/necessary ...] to [him / other NPs]


<H3> Bitexts generated by machine translation </H3>
<A href="http://www.cs.jhu.edu/~juri/pdf/watermarking-emnlp-2011.pdf">
Watermarking the Outputs of Structured Prediction with an application
in Statistical Machine Translation </A>
A. Venugopal et al., EMNLP 2011.

<p>
Observation: In some languages, half of the bitexts that can be gathered
were created by machine translation.
E.g. 50.6% in Tagalog; 44.5% in hindi, 41.9% in Galician.
<p>
This obviously creates opportunity for destructive positive feedback
in MT based on bitexts.
<p>
Extremely clever technique for probabilistically watermarking the outputs 
that one's own translation program produced, based on choosing synonyms.
The watermarks are still identifiable, even if the translation program 
changes.

<!--
<p>
<A href=
"http://repository.dlsi.ua.es/251/1/workshops/W19_Proceedings.pdf#page=53"> 
Introducing and evaluating ukWac, a very large web-derived corpus of 
English </A>
Adriano Ferraresi et al. <em> Proceedings of the 4th Web as Corpus 
Workship</em> 2008.

<p>
<A href=
"http://www.springerlink.com/content/c348pu7321gx5081/">
The WaCKy wide web: A collection of very large linguistically procesed 
web-crawled corpora </A> Marco Baroni et al, Language Resources
and Evaluations, 2009.




<A href="http://www.cis.uni-muenchen.de/people/langer/veroeffentlichungen/bulag.pdf"> Natural Languages and the World Wide Web
</A>
Stefan Langer


<p>
<A href="http://citeseer.ist.psu.edu/444684.html">
Building Minority Language Corpora by Learning to Generate Web Search Queries
</A> Rayid Ghani, Rosie Jones, and Dunja Mladenic


<p>
<A href="http://www.kilgarriff.co.uk/Publications/2003-K-LSEsprolac.pdf">
Linguistic Search Engine
</A> Adam Kilgarriff

<p>
<A href=
"http://www.kilgarriff.co.uk/Publications/2006-BaroniKilg-EACL-DeWAC.pdf"> 
Large linguistically-processed Web corpora for multiple languages
</A> Marco Baroni and Adam Kilgarriff

<p>
<A href="http://citeseer.ist.psu.edu/454411.html">
Exploiting the WWW as a corpus to resolve PP attachment ambiguities
</A> Martin Volk

<p>
<A href="http://acl.ldc.upenn.edu/acl2004/emnlp/pdf/Szpektor.pdf">
Scaling Web-based Acquisition of Entailment Relations 
</A> Idan Szpektor et al.


<p>
<A href="http://portal.acm.org/citation.cfm?id=1075392">
Web-based models for natural language processing
</A> Mirella Lapata and Frank Keller

<p>
<A href="http://portal.acm.org/citation.cfm?id=1075392">
Mining the Web for Discourse Markers
</A> Ben Hutchison



<p>
<A href="http://doi:10.1016/j.ipm.2004.06.006"> 
Cross-Language Information Retrieval: the way ahead
</A>Fredric Gey, Noriko Kando, and Carol Peters

Online Information Review, Vol. 32 Iss: 5, pp.668 - 672


<p>
<A href="http://anthology.aclweb.org/Y/Y10/Y10-1069.pdf">
AUTOLEX: An Automatic Lexicon Builder for Minority Languages Using an 
Open Corpus. </A>
E. Buhay et al.  PACLIC. Vol. 10. 2010.

<p>
<A href="http://www.ijcsi.org/papers/IJCSI-8-3-1-47-58.pdf">
Language Identification of Web Pages based on Improved N-gram Algorithm,
</A>
Y. Chew, Y. Mikami, R. Nagano. (2011)
International Journal of Computer Science Issues (IJCSI) 8.3.
-->
