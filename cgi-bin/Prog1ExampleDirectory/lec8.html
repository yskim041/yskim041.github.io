
<TITLE> Lecture 8: Invisible Web; Tables; Sentiment Analysis </H2>
</TITLE>
<H2> Lecture 8: Invisible Web; Tables; Sentiment Analysis </H2>

<H3> Required reading </H3>
<A href="http://portal.acm.org/citation.cfm?id=1897839">
Structured Data on the Web </A>, M.J. Cafarella, A. Halevy, and J. Madhavan,
<em>CACM,</em> 54:2, Feb. 2011. 
<p>
<A href="http://portal.acm.org/citation.cfm?id=1454163">
Google's Deep Web crawl </A> Madhavan et al. 
<em>Proc. VLDB Endowment</em>, 1:2,
August 2008. <br>
This is the central, current article on the topic, but unfortunately very
poorly written. Do what you can with it.
<p>
<A href="http://www.eecs.umich.edu/~michjc/papers/webtables_vldb08.pdf">
WebTables: Exploring the Power of Tables on the Web,</A>
Michael Cafarella et al.
<p>
<A href=
"http://dl.acm.org/citation.cfm?id=2433442"> Crawling deep web entity pages
</A> He et al.



<H3> More Reading </H3>
<A href="http://www.personal.psu.edu/faculty/p/u/pum10/www08-bsun.pdf">
Mining, Indexing, and Searching for Textual Chemical Molecule Information
on the Web</A>, Bingjun Sun, Prasenjit Mitra, and C. Lee Giles,
WWW 2008.  <br>
<A href="http://www-db.cs.wisc.edu/cidr/cidr2009/Paper_115.pdf"> 
Harnessing the Deep Web: Present and Future</A> J. Madhavan et al.,
<em>CIDR (Conference on Innovative Data Systems Research), 2009. </em>  <br>
<A href="http://portal.acm.org/citation.cfm?id=1860708">
Understanding deep web search interfaces: A survey.</A>
Ritu Khare, Yuan An, and Il-Yeol Song, <em> ACM SIGMOD Record</em> 39:1,
March 2010. <br>
<A href="http://www.eecs.umich.edu/~michjc/papers/webtables_webdb08.pdf">
Uncovering the Relational Web,</A> Michael Cafarella et al., WebDB 2008. <br>

<H3> Older but still interesting </H3>
 <A href="http://citeseer.ist.psu.edu/708823.html">
Structured Databases on the Web: Observations and Implications
</A> Kevin Chang et. al <br>
<A href="http://www.emeraldinsight.com/journals.htm?articleid=1509085">
Indexing the
invisible web: a survey </A> by Yanbo Ru and Ellis Horowitz, 2005. <br>
<A href="http://www.press.umich.edu/jep/07-01/bergman.html">
The Deep Web: Surfacing Hidden Value </A>
by Michael K. Bergman, Journal of Electronic Publishing <br>
 <A href="http://ilpubs.stanford.edu:8090/725/">
Crawling the Hidden Web </A>  
Sriram Raghavan, Hector Garcia-Molina, <em>VLDBM</em> 2001. <br>



<H3> Invisible Web </H3>
<b> Indexable Web: </b> Pages that can be found by crawling  <br>
<b> Invisible Web </b> (Hidden web, deep web):
Information that can be accessed only or primarily by filling out a form.
browser, but will not be found by a standard crawler for various reasons.

<p>  
<b> An enormous invisible Web site,</b> though not a very interesting one: <br>
<A href="http://www.ncdc.noaa.gov/oa/ncdc.html">
NOAA (National Oceanic and Atmospheric Administration)
National Climatic Data Center</A>
Doppler Weather Radar Data: 1600 TBytes (as of 2006) <br>
In 2007 NOAA projected that they will be storing 140 PetaBytes by 2020. <br>
I can't find a current (2011) count.

<H4> Surfacing Deep Invisible Web </H4>
Madhavan et al., "Google's Deep-Web Crawl"

<p>
Basic idea: Execute forms. Get pages. Add to Google index just as if they
were ordinary static web pages. "Surfaced pages contribute results to more
than 1,000 Web-search queries per second on Google.com". Since Google receives
about 34,000 per query, then assuming that "contribute results" means "in
a results page of 10 items", that means that 1 out of 340 results is from
the surfaced web.

<p>
Emphasis on getting some data from a lot of sites rather than exhaustively
surfacing a single site.

<UL>
<LI> Locate HTML forms with action "get".
<LI> Find informative templates.
<LI> Issue queries corresponding to templates. Fill in text for textboxes.
<LI> Index results.
</UL>

<P>
<B> Template: </b> When you fill out a form, some of the input fields are <em>
binding </em> (that is, have a specified value) and the rest are <em> free</em>
("Any" option, blank text, wildcard). Some inputs are presentational (e.g.
order); these are unimportant.
A <em>template</em> is a choice of
binding fields. The <em> dimension</em> of the template is the number of 
binding fields. 

<p>
<b> Informativeness: </b> A <em>signature</em> for a results page is a way of
computing whether two pages are the same: Ignore HTML formatting, ads, order
of results. Delete query words from page (just echoing).
Queries that give HTML errors or give failed search messages 
are ignored. The absolute <em>informativeness</em> of template T is 
(the number of distinct signatures returned in queries generated by T) /
(the number of form submissions on T). If the informativeness is greater than
0.2 then T is informative.

<p>
The incremental informativeness is the number of <em> new</em> distinct
signatures divided by the number of queries.


<p>
<b> Generating templates </b> <br>
Try all 1-dimensional templates. <br>
For each, issue a lot of queries. <br>
See which templates are informative. <br>
Extend all the 1-D templates to 2-D templates <br>
Iterate. Stop after 3-D templates. 

<p>
<b>Generating queries.</b> Here things become murky.
<UL>
<LI>
Finite choice (radio button, drop down menu, etc.). Try all.
<LI>
Typed text box. Known collection of types. US Cities, Zip Code, Lowest-Price,
Highest-Price, Zip Code. It is not clear to me whether these are all the
types that the system actually uses or just those for which they did some
careful experimentation. Judge type from (a) associated name (b) values that
work.
<LI> Generic text box. Get a seed set of query words off the front form itself.
Issue queries, mine the results pages for words with high TF-IDF, which do
not appear on every results page and which appear on more than one results
page. Iterate.
</UL>

<p>
<b> Experimentation and evaluation: </b> I am skipping the experimentation
except for Table 4, in which they compare the 
number of records that the system against the number of items in the database,
for a dozen databases where this number was published or determinable.
In all but one case, they were able to retrieve most of the records. 
What's strange are the two databases with 27 records.



<H3> WebTables </H3>
Work in progress: Prototype systems developed, but not available to the 
end-user.

<p>
<b> Objective: </b> Get useful information out of vast number of relational
tables on Web.

<p>
14.1 billion HTML tables in English documents in Google index (2008). <br>
About 154 Million of these are actually relational tables (rest are for 
formatting etc.)
<H4> Offline </H4>
<UL>
<LI> Extract true relational tables out of HTML tables using features. <br>
e.g. 1 column or 1 row -- No good. <br>
columns heterogeneous in type -- probably no good. <br>
Use ML techniques to build classification filter over similar structural
features.

<LI>
Extract metadata: name of attributes. Test on features to determine whether
first row is list of attributes.

<LI> 
Construct ACSDb (Attribute Correlation Statistics Database): list of all
sets of attributes.

<LI>
Compute absolute probabilities P(a) and conditional probabilities P(a|b) that
an attribute will appear in a table. 

</UL>


<H4> Applications </H4>
<p>
<b> Table Search Engine. </b>
Give high rank to tables that match query in top row and in leftmost column. 
Give query-indendent high rank to tables with high "Schema coherency"
of the attributes, based on the ACSDb.

<p>
<b> Schema Auto-Complete. </b> A web designer enters some of the attributes of
a table. Auto-complete suggests some more, based on high conditional 
probability.

<p>
<b> Attribute synonymy.</b>
Attribute A is a synonym for attribute B if 
<UL>
<LI>
A and B never appear together in the same schema.
<LI> 
For most other attributes C, P(C|A) and P(C|B) are approximately equal.
</UL>

<H3> Metasearch </H3>
Used to be that there were many (a dozen) general search engine, with 
surprisingly little overlap. So for a thorough search you wanted to consult
them all. Metasearch automatically sent your query to all, then collated the
results.  It was unpopular with the search engines, because it stripped the 
ads.
Issues:
<UL>
<LI> Query reformulation, if the advanced options were different.
<LI> Identify duplicate pages.
<LI> Collate rankings.
<LI> Collate snippets.
</UL>


<H3> Sentiment Analysis </H3>
Lots of opinions about lots of things on the web.

<p> <b> Level of analysis </b>
<UL>
<LI> Document-Level Sentiment Classification
<LI> Sentence-Level Sentiment Classification
<LI> Feature-Based Sentiment Analysis
<LI> Opinion Search Engine
</UL>

<p> <b> Sub-tasks </b>
<UL>
<LI> Does the text express an opinion?
<LI> Whose opinion?
<LI> About what?
<LI> Favorable or unfavorable?
<LI> Spam detection
</UL>

<p> <b> 2 nice feature of document level classification </b> <br>
1) There is an immense body of free labelled data, in the form of reviews 
that have both a text and a number of stars.  <br>
2) Unlike most natural language tasks, the output is a number between 1 and 5.

<p> <b> Supervised classification learning </b> <br>
Classification problem: A universe of instances. A finite set of categories,  
a numerical measure. Given a new instance, predict which category it
belongs to, or what is the value of the measure. <br>
Supervised learning. You have 
available a corpus of instances that are labelled correctly, or mostly 
correctly. <br>
Learning task: Learn a classifier from the labelled instances. Then do
classification by applying the classifier to the new instance. 

<p>
I'm not going to teach techniques for supervised classification learning.
We will simply assume that these techniques exist and work better or worse
depending on the problem. For a little bit about this, take the AI class
G22.2560; for a lot about it, take "Machine Learning" G22.2565. 
<p>
What I will discuss are the <em> features </em> that are used for 
classification, and techniques that go beyond supervised classification.


<H4> Depth of the Natural Language Analysis </H4>
To do the problem with perfect accuracy would require a perfect natural
language understander, which is not available. The question is what can you 
actually accomplish.

<p>
I. Some words (mostly adjectives; to a lesser extent nouns, verbs,
and adverbs) are favorable ("good", "amazing", "beautiful")
and some are unfavorable ("bad", "dreadful", "junk").

<p>
So for a very crude analysis you use a classifier that does classification
just based on 
these words (e.g. by a weighted sum). Learn the classifier from
the labelled corpus. "Applying machine learning techniques based on unigram
[single word] models achieve over 80% in accuracy" (Pang and Lee p. 19)
(for task of distinguishing positive from negative document). 

<p>
II.A.  Learn new words via synonyms/antonyms on WordNet. Learn word in foreign
language from dictionary.

<p>
II.B. Learn new words via connectives. "U and V" -- probably same polarity.
"U but V" --- probably opposite polarity.

<p>
III. Simple phrasal context can change the force of a word. Most notably
"not W" has (generally) the reverse polarity from W. But you have to be 
careful of phrases like "not only W but also U", for which W has its positive
polarity. Note the contrast with topic relevance. Quite a few other words
with a negative force e.g. "It avoids all cliches and predictability
found in Hollywood movies" (example from Pang and Lee, p. 36).


<p>
IV. Feature analysis.
<UL>
<LI> The sentence may name the feature explicitly e.g. "the picture quality 
is amazing" or implicitly e.g. "the camera is too expensive" where the
implicit feature is price.
<LI> Same feature may have multiple names. Try WordNet synonym.
<LI> Figuring out which adjective goes with which feature can be done some
percentage of the time by part-of-speech labelling, then matching 
simple patterns. E.g. "< Noun > is < Adjective >". Doing this better gets
into hard NLP problems such as parsing and coreference (pronouns). 
<LI> Some characteristics are good for some features and bad for others.
E.g. "terrifying" is a virtue in horror movies and a defect in power tools.
"Short" is a virtue in booting times and a defect in battery life.
So you can do this by discriminating more finely on your feature set, but 
hard to attain a high degree of accuracy.
</UL>

<p>
V. Discourse Analysis.
"In newsgroups devoded to three distinct controversial topics (abortion,
gun control, and immigration) Agrawal et al. observe that 
The relationship between two individuals in the `responded to' network
is more likely to be antagonistic --- overall 74% of the responses 
examined were found to be antagonistic, whereas only 7% were found to
be reinforcing." (Pang and Lee p. 48).

<p>
Cocitation --- two blogs that cite the same source -- tends to indicate
agreement.

<p>
<A href="http://portal.acm.org/citation.cfm?id=775227">
Mining Newsgroups Using Networks Arising from Social Behavior.
</A> Rakesh Agrawal et al.

<p>
Observation: In a newsgroup, if X quotes Y, it is much more often to
disagree than to agree (74% to disagree, 7% to agree, 19% off topic).

<p>
Therefore: Construct the graph whose nodes are posters and where
there is an arc from X to Y if X quotes Y. Divide the set of posters
into two disjoint classes F (for) and A (against) in such a way that
the maximal number of arcs go from one class to the other.

<p>
This problem is known as "MAX-CUT" and is NP-complete, but there are
approximation algorithms.

<p> <b> Experiment: </b> Three newsgroups: abortion (2525 authors),
gun control (2632 authors), and immigration (1648 authors).

<p> Two versions of the algorithms: the pure version and the "constrained"
version where 50 random authors were manually labelled, and the maximal
cut respecting that labelling was returned.

<p>
Compare text-based SVM and Naive Bayes: both useless.

<p align=center>
<table border=1>
<tr> <th> Algorithm <th> Abortion <th> Gun Control <th> Immigration
<tr>  <td> Majority assignment <td> 57% <td> 72% <td> 54%
<tr> <td> SVM <td> 55 <td> 42 <td> 55
<tr> <td> Naive Bayes <td> 50 <td> 72 <td> 54
<tr> <td> EV <td>  73 <td> 78 <td> 50 <td>
<tr> <td> Constrained EV <td> 73 <td> 84 <td> 88
</table>



<p>
VI. Discriminating opinions from other text. Direct statements of subjectivity
("In my opinion") or strong presence of evaluative words is suggestive but
not decisive in either direction. Odd fact: "<em>Hapax legomena</em> or words
that appear a single time in a given corpus have been found to be 
high-precision indicators of subjectivity" (Pang and Lee p. 33).

<p>
VII. Opinion search engine. Differs from the usual objective of a search engine
in that 
<UL>
<LI> You need to extract documents that express an opinion about 
a  given subject; not a category in a
usual search engine.
<LI> You would like to extract the polarity of the opinion and even to refine 
it in terms of the feature.
<LI> You are not looking for a single best document or a few best documents.
You are looking for the count of positive vs. negative opinions or something
of the kind. So ranking largely goes away, and therefore so do 
query-independent measures of importance. The important measures are 
recall of relevant opinionated documents and precision of the 
categorization, both as opinionated and of polarity. This raises the obvious
issue of:
<LI> Presentation of results. If a large number of opinionated documents have
been found, what is the best way to present a summary? Long discussion at
Pang and Lee.
<LI> Quality of review. Again, there is sometimes feedback from other reviewers\so this can be treated as a preference analysis problem at the metalevel.
<LI> Two kinds of users: Purchasers and Marketers. Needs are a little different.
</UL>
<p>
VIII. Spam, both positive and negative. Very hard to detect. Liu proposes as a 
sufficient test cases (a) where the same review has multiple userid's and (b)
where the same review applies to different items. But this gives a very limited
corpus which may not be representative. Liu observes that spam seems to gather 
<em> more </em> "this review was helpful" metacomments than non-spam, but of
course these metacomments may also be spam.


